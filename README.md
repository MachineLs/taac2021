# taac2021

# REFERENCES
## Models
1. [FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval](https://arxiv.org/ftp/arxiv/papers/2005/2005.09801.pdf)

    [Github](https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert)
  
2. [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557.pdf)

    [Github](https://github.com/uclanlp/visualbert)
    
3. [CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis](https://github.com/thuiar/Cross-Modal-BERT/blob/master/paper/ACM%20MM2020.pdf)

    [Github](https://github.com/MSJYYT/Cross-Modal-BERT)

## Fusion
1. [Deep Multimodal Fusion by Channel Exchanging](https://papers.nips.cc/paper/2020/file/339a18def9898dd60a634b2ad8fbbd58-Paper.pdf)

    [Github](https://github.com/yikaiw/CEN)
    
2. [Image and Text fusion for UPMC Food-101 using BERT and CNNs](http://artelab.dista.uninsubria.it/res/research/papers/2020/2020-IVCNZ-Gallo-Food101.pdf)

    [Github](https://github.com/artelab/Image-and-Text-fusion-for-UPMC-Food-101-using-BERT-and-CNNs)
    
3. [VD-BERT: A Unified Vision and Dialog Transformer with BERT](https://arxiv.org/pdf/2004.13278.pdf)

    [Github](https://github.com/salesforce/VD-BERT)
    
4. [Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment](https://arxiv.org/pdf/2012.02813.pdf)

   [Github](https://github.com/peter-yh-wu/xmodal)
